{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright **`(c)`** 2023 Giovanni Squillero `<giovanni.squillero@polito.it>`  \n",
    "[`https://github.com/squillero/computational-intelligence`](https://github.com/squillero/computational-intelligence)  \n",
    "Free for personal or classroom use; see [`LICENSE.md`](https://github.com/squillero/computational-intelligence/blob/master/LICENSE.md) for details.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB10\n",
    "\n",
    "Use reinforcement learning to devise a tic-tac-toe player.\n",
    "\n",
    "### Deadlines:\n",
    "\n",
    "* Submission: [Dies Natalis Solis Invicti](https://en.wikipedia.org/wiki/Sol_Invictus)\n",
    "* Reviews: [Befana](https://en.wikipedia.org/wiki/Befana)\n",
    "\n",
    "Notes:\n",
    "\n",
    "* Reviews will be assigned  on Monday, December 4\n",
    "* You need to commit in order to be selected as a reviewer (ie. better to commit an empty work than not to commit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1253,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from collections import namedtuple, defaultdict\n",
    "from copy import deepcopy\n",
    "from random import choice\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the State namedtuple\n",
    "State = namedtuple('State', ['x', 'o'])\n",
    "MAGIC = [2, 7, 6, 9, 5, 1, 4, 3, 8]\n",
    "\n",
    "def print_board(pos):\n",
    "    \"\"\"Nicely prints the board\"\"\"\n",
    "    for r in range(3):\n",
    "        for c in range(3):\n",
    "            i = r * 3 + c\n",
    "            if MAGIC[i] in pos.x:\n",
    "                print('X', end='')\n",
    "            elif MAGIC[i] in pos.o:\n",
    "                print('O', end='')\n",
    "            else:\n",
    "                print('.', end='')\n",
    "        print()\n",
    "    print()\n",
    "\n",
    "def win(elements):\n",
    "    \"\"\"Checks if elements are winning\"\"\"\n",
    "    return any(sum(c) == 15 for c in combinations(elements, 3))\n",
    "\n",
    "def state_value(pos: State):\n",
    "    \"\"\"Customized state evaluation\"\"\"\n",
    "    # Check if X wins\n",
    "    if win(pos.x):\n",
    "        return 1  # Positive value if X wins\n",
    "    # Check if O wins\n",
    "    elif win(pos.o):\n",
    "        return -1  # Negative value if O wins\n",
    "    else:\n",
    "        return 0  # Return 0 for a draw\n",
    "\n",
    "def choose_action(state, available_actions, q_values):\n",
    "    # Create a hashable representation of the current state\n",
    "    hashable_state = (tuple(sorted(state.x)), tuple(sorted(state.o)))\n",
    "\n",
    "    # Get Q-values for the current state from the Q-table\n",
    "    q_values_state = q_values.get(hashable_state, {})\n",
    "\n",
    "    # If no Q-values are available, choose a random action\n",
    "    if not q_values_state:\n",
    "        return choice(available_actions)\n",
    "\n",
    "    # Select the action with the maximum Q-value\n",
    "    available_q_values = {action: q_values_state.get(action, 0) for action in available_actions}\n",
    "    return max(available_q_values, key=available_q_values.get)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reiforcement Learning Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a global cache for memoization\n",
    "from copy import deepcopy\n",
    "\n",
    "min_cache = {}\n",
    "max_cache = {}\n",
    "\n",
    "def minmax(state, player, maximize):\n",
    "    cache = max_cache if player == maximize else min_cache\n",
    "    \n",
    "    possible = list(set(range(1, 10)) - state.x - state.o)\n",
    "\n",
    "    state_key = (tuple(sorted(state.x)), tuple(sorted(state.o)))\n",
    "    if state_key in cache:\n",
    "        return cache[state_key]\n",
    "    \n",
    "    over = state_value(state)\n",
    "    if over != 0 or not possible:\n",
    "        if maximize == \"o\":\n",
    "            over = -over\n",
    "        return None, over\n",
    "    \n",
    "    scores = []\n",
    "    for i, move in enumerate(possible):\n",
    "        new_g = deepcopy(state)\n",
    "        if player == \"x\":\n",
    "            new_g.x.add(move)\n",
    "            scores.append((i, minmax(new_g, \"o\", maximize)[1]))\n",
    "        else:\n",
    "            new_g.o.add(move)\n",
    "            scores.append((i, minmax(new_g, \"x\", maximize)[1]))\n",
    "    \n",
    "    if player == maximize:  # max player\n",
    "        best = max(scores, key=lambda x: x[1])\n",
    "    else:\n",
    "        best = min(scores, key=lambda x: x[1])\n",
    "    \n",
    "    choice = possible[best[0]]\n",
    "    cache[state_key] = (choice, best[1])\n",
    "    return choice, best[1]\n",
    "\n",
    "def backpropagation(Qtable: dict, states: list, actions: list, reward, learning_rate, gamma, default=0):\n",
    "    # Initialize the next state as None\n",
    "    next_state = None\n",
    "    # Iterate over the reversed list of states and actions\n",
    "    for act, state in zip(reversed(actions), reversed(states)):\n",
    "        # Calculate the maximum Q-value for the next state\n",
    "        fut_max = max(Qtable.get(next_state, {}).values()) if next_state and Qtable.get(next_state) else default\n",
    "        # Update the Q-value for the current state and action\n",
    "        Qtable[state][act] = Qtable[state].get(act, 0) + learning_rate * (reward + gamma * fut_max - Qtable[state].get(act, 0))\n",
    "        # Update the next state for the next iteration\n",
    "        next_state = state\n",
    "    \n",
    "    return Qtable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training phase using Random and MinMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1256,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:02<00:00, 9399.39it/s] \n"
     ]
    }
   ],
   "source": [
    "def q_learning(Qtable, alpha, gamma):\n",
    "    # Initialize an empty state for the tic-tac-toe game\n",
    "    state = State(set(), set())\n",
    "    # Define the set of available moves\n",
    "    available = set(range(1, 10))\n",
    "\n",
    "    states = []  # List to store states encountered during the game\n",
    "    actions = []  # List to store actions taken during the game\n",
    "    first_player = choice((0, 1))  # Randomly choose the first player\n",
    "    while available:\n",
    "        # Choose action based on Q-values for the first player, random for the second player\n",
    "        if first_player == 0:\n",
    "            action, _ = minmax(state, \"x\", \"x\")\n",
    "        else:\n",
    "            action = choice(list(available))\n",
    "\n",
    "        # Create a copy of the current state\n",
    "        next_state = State(state.x.copy(), state.o.copy())\n",
    "        # Update the state with the chosen action\n",
    "        next_state.x.add(action) if first_player == 0 else next_state.o.add(action)\n",
    "        # Remove the chosen action from the set of available actions\n",
    "        available.remove(action)\n",
    "\n",
    "        # Calculate the reward for the next state\n",
    "        \n",
    "\n",
    "        # Add the current state and action to the lists\n",
    "        states.append((tuple(sorted(state.x)), tuple(sorted(state.o))))\n",
    "        actions.append(action)\n",
    "\n",
    "        # Update the current state based on the chosen action\n",
    "        state.x.add(action) if first_player == 0 else state.o.add(action)\n",
    "\n",
    "        # Check for a win or if there are no more available moves\n",
    "        if win(state.x) or win(state.o) or not available:\n",
    "            break\n",
    "\n",
    "        first_player = 1 - first_player\n",
    "       \n",
    "    # Perform backpropagation to update Q-values based on the game trajectory\n",
    "    reward = state_value(state)\n",
    "    Qtable = backpropagation(Qtable, states, actions, reward, alpha, gamma)\n",
    "    return Qtable\n",
    "\n",
    "# Initialize Q-values dictionary\n",
    "q_values = defaultdict(lambda: defaultdict(float))\n",
    "# Set learning rate (alpha) and discount factor (gamma)\n",
    "alpha = 0.1\n",
    "gamma = 0.9\n",
    "\n",
    "# Run the Q-learning algorithm for a specified number of steps\n",
    "for steps in tqdm(range(20_000)):\n",
    "    q_values = q_learning(q_values, alpha, gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1257,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 20045.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of wins for the Q-learning agent: 81.17%\n",
      "Percentage of wins for the opponent: 2.03%\n",
      "Percentage of draws: 16.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def play_game(q_values):\n",
    "    # Initialize an empty state for the tic-tac-toe game\n",
    "    state = State(set(), set())\n",
    "    # Define the set of available moves\n",
    "    available = set(range(1, 9 + 1))\n",
    "\n",
    "    # Randomly choose the first player (0 or 1)\n",
    "    first_player = choice((0,1))\n",
    "\n",
    "    while available and state_value(state) == 0:\n",
    "        if first_player == 0:\n",
    "            # If it's the first player's turn, choose the action using Q-values\n",
    "            x = choose_action(state, list(available), q_values)\n",
    "            state.x.add(x)\n",
    "        else:\n",
    "            # If it's the second player's turn, choose a random action\n",
    "            o = choice(list(available))\n",
    "            #It is possible to challenge also the min max\n",
    "            #o, _ = minmax(state, \"o\", \"x\")\n",
    "            state.o.add(o)\n",
    "\n",
    "        # Remove the chosen action from the set of available actions\n",
    "        available.remove(x if first_player == 0 else o)\n",
    "        # Check if the first player wins\n",
    "        if win(state.x):\n",
    "            # Return 1 if the first player wins\n",
    "            return 1\n",
    "        # Check if the second player wins\n",
    "        if win(state.o):\n",
    "            # Return -1 if the second player wins\n",
    "            return -1\n",
    "        # Check if the game is a draw\n",
    "        if not available:\n",
    "            # Return 0 if the game is a draw\n",
    "            return 0\n",
    "\n",
    "        # Switch the player for the next turn\n",
    "        first_player = 1 - first_player\n",
    "\n",
    "# Set the number of games to simulate\n",
    "num_games = 10000\n",
    "count_q = 0\n",
    "count_r = 0\n",
    "count_d = 0\n",
    "\n",
    "# Run the simulation for the specified number of games\n",
    "for _ in tqdm(range(num_games)):\n",
    "    res = play_game(q_values)\n",
    "    if res == 1:\n",
    "        count_q += 1\n",
    "    elif res == -1:\n",
    "        count_r += 1\n",
    "    else:\n",
    "        count_d += 1\n",
    "\n",
    "# Print the results of the simulation\n",
    "print(f\"Percentage of wins for the Q-learning agent: {(count_q/num_games)*100:.2f}%\")\n",
    "print(f\"Percentage of wins for the opponent: {(count_r/num_games)*100:.2f}%\")\n",
    "print(f\"Percentage of draws: {(count_d/num_games)*100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ci-fLJ3OwGs-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
